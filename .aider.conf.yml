## Specify the model to use for the main chat
# Dense models
# model: ollama_chat/qwen2.5:7b
# model: ollama_chat/gemma3:12b
# model: ollama_chat/gemma3:4b

#  Section containing Mixture of experts model (the less active parameters the more able to run on the CPU)
# mixture of experts model
model: ollama_chat/qwen3:30b-a3b


## Specify the model to use for commit messages and chat history summarization (default depends on --model)
# weak-model: ollama/qwen2.5:7b


# Server Configuration property does not exist exist
# We need to export OLLAMA_API_BASE=http://127.0.0.1:11434 
# server: http://localhost:11434
# The Yaml configuration is open-ai-base
## Specify the api base url
openai-api-base: http://localhost:11434
# ollama-api-base: http://localhost:11434

###############
# Git Settings:

## Enable/disable looking for a git repo (default: True)
git: true

## Enable/disable adding .aider* to .gitignore (default: True)
gitignore: false

## Specify the aider ignore file (default: .aiderignore in git root)
aiderignore: .aiderignore

## Only consider files in the current subtree of the git repository
#subtree-only: false

## Enable/disable auto commit of LLM changes (default: True)
auto-commits: false

## Run aider in your browser
#gui: false

## Enable/disable suggesting shell commands (default: True)
#suggest-shell-commands: true

set-env:
  - OLLAMA_API_BASE=http://127.0.0.1:11434 

